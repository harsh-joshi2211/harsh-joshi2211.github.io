<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hand Tracking System | Harsh Joshi</title>
  <link rel="stylesheet" href="../style.css">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&family=Fira+Code&display=swap" rel="stylesheet">
</head>
<body>
  <header class="page-header">
    <h1>Hand Tracking System</h1>
    <a href="../projects.html" class="back-link">Back to Projects</a>
  </header>

  <div class="page-content">
    <article class="project-detail">
      <h2>Project Overview</h2>
      <p>This real-time hand tracking system leverages MediaPipe and OpenCV to detect and track 21 3D hand landmarks at over 30 FPS on consumer hardware. The system enables gesture-based interaction for applications ranging from sign language interpretation to immersive AR/VR controls.</p>
      
      <h2>Technical Architecture</h2>
      <p>The pipeline consists of multiple optimized stages:</p>
      <ul>
        <li><strong>Input Processing:</strong> Adaptive frame sampling based on system load</li>
        <li><strong>Detection Phase:</strong> Palm detection using BlazePalm model</li>
        <li><strong>Landmark Estimation:</strong> 3D coordinate prediction for 21 hand joints</li>
        <li><strong>Gesture Classification:</strong> Dynamic time warping for sequence recognition</li>
      </ul>
      
      <h2>Performance Metrics</h2>
      <table>
        <tr>
          <th>Metric</th>
          <th>Value</th>
        </tr>
        <tr>
          <td>Inference Time</td>
          <td>8ms per frame (GTX 1060)</td>
        </tr>
        <tr>
          <td>Tracking Accuracy</td>
          <td>94.2% (on EgoHands dataset)</td>
        </tr>
        <tr>
          <td>Latency</td>
          <td>42ms end-to-end</td>
        </tr>
      </table>
      
      <h2>Innovative Features</h2>
      <p>The system includes several novel improvements:</p>
      <ul>
        <li>Adaptive region-of-interest detection to minimize processing area</li>
        <li>Landmark trajectory smoothing using Kalman filters</li>
        <li>Gesture conflict resolution for ambiguous cases</li>
        <li>Dynamic confidence thresholding based on lighting conditions</li>
      </ul>
      
      <h2>Applications Developed</h2>
      <ol>
        <li><strong>Virtual Mouse:</strong> Control computer cursor with hand gestures</li>
        <li><strong>Sign Language Translator:</strong> ASL fingerspelling recognition</li>
        <li><strong>3D Modeling Interface:</strong> Gesture-based CAD manipulation</li>
      </ol>
      
      <h2>Optimization Techniques</h2>
      <pre><code># Landmark post-processing
def smooth_landmarks(current, previous):
    # Kalman filter implementation
    kalman_gain = estimate_kalman_gain(current, previous)
    smoothed = []
    for i in range(21):
        x = kalman_gain * current[i].x + (1-kalman_gain)*previous[i].x
        y = kalman_gain * current[i].y + (1-kalman_gain)*previous[i].y
        z = kalman_gain * current[i].z + (1-kalman_gain)*previous[i].z
        smoothed.append(Landmark(x,y,z))
    return smoothed
</code></pre>
      
      <h2>Challenges Overcome</h2>
      <p>Key technical challenges addressed:</p>
      <ul>
        <li>Occlusion handling during rapid hand movements</li>
        <li>Lighting condition variability</li>
        <li>CPU/GPU resource management</li>
        <li>Cross-platform compatibility</li>
      </ul>
      
      <a href="../projects.html" class="back-link">Back to Projects</a>
    </article>
  </div>
</body>
</html>